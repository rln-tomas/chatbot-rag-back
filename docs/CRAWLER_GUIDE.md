# ğŸ•·ï¸ GuÃ­a del Web Crawler

## ğŸ“‹ DescripciÃ³n

El sistema de scraping ahora incluye un **crawler recursivo** que descubre y extrae contenido de todas las pÃ¡ginas dentro del mismo dominio.

## âœ¨ CaracterÃ­sticas

### âœ… Lo que hace el crawler:

1. **Descubrimiento automÃ¡tico de pÃ¡ginas**: Comienza desde una URL y descubre todos los enlaces
2. **Respeta el dominio**: Solo visita pÃ¡ginas del mismo dominio (no sale del sitio)
3. **NavegaciÃ³n recursiva**: Sigue enlaces de forma inteligente
4. **Filtrado inteligente**: Ignora archivos descargables (PDFs, imÃ¡genes, videos, etc.)
5. **LÃ­mite de seguridad**: Por defecto, mÃ¡ximo 50 pÃ¡ginas (configurable)
6. **NormalizaciÃ³n de URLs**: Evita duplicados por URLs similares
7. **Manejo de errores**: ContinÃºa aunque falle alguna pÃ¡gina

### âŒ Lo que NO hace:

- No sale del dominio especificado
- No descarga archivos binarios (PDFs, imÃ¡genes, etc.)
- No ejecuta JavaScript (solo extrae HTML estÃ¡tico)
- No sigue enlaces externos

## ğŸš€ Uso

### OpciÃ³n 1: A travÃ©s de la API (Recomendado)

```bash
# 1. Crear una configuraciÃ³n con la URL
curl -X POST "http://localhost:8000/api/v1/config/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://tu-sitio-web.com",
    "name": "Mi Sitio Web"
  }'

# 2. Iniciar el scraping (automÃ¡ticamente crawlea todo el sitio)
curl -X POST "http://localhost:8000/api/v1/scraping/start" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "config_id": 1
  }'
```

### OpciÃ³n 2: Script de prueba

Para probar el crawler sin la API completa:

```bash
# Probar con una URL especÃ­fica
python scripts/test_crawler.py https://tu-sitio-web.com
```

El script mostrarÃ¡:

- URLs descubiertas
- PÃ¡ginas scrapeadas
- Chunks generados
- EstadÃ­sticas de contenido

## âš™ï¸ ConfiguraciÃ³n

Puedes ajustar el comportamiento del crawler en `app/scraping/tasks.py`:

```python
scraper = WebScraper(
    chunk_size=2000,        # TamaÃ±o de cada chunk de texto (aumentado)
    chunk_overlap=400,      # Solapamiento entre chunks (aumentado)
    max_pages=50,           # MÃ¡ximo de pÃ¡ginas a crawlear
    timeout=10             # Timeout por request en segundos
)
```

## ğŸ“Š Ejemplo de Salida

Cuando ejecutas un scraping, verÃ¡s algo como:

```
Starting crawl of example.com from https://example.com
Crawling [1/50]: https://example.com
Crawling [2/50]: https://example.com/about
Crawling [3/50]: https://example.com/products
...
Crawl completed. Discovered 15 pages.

Starting scraping of 15 discovered pages...
Scraping [1/15]: https://example.com
  âœ“ Extracted 8 chunks
Scraping [2/15]: https://example.com/about
  âœ“ Extracted 4 chunks
...
Scraping completed. Total chunks: 145

Embedding 145 chunks into Pinecone...
Embedded batch 1 of 2
Embedded batch 2 of 2
```

## ğŸ” CÃ³mo Funciona

### 1. **Fase de Descubrimiento**

```
URL inicial â†’ Extrae todos los enlaces â†’ Filtra por dominio â†’ Cola de URLs
     â†“                                                              â†“
  Visita â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Fase de Scraping**

```
Lista de URLs â†’ Scrape cada pÃ¡gina â†’ Divide en chunks â†’ Guarda en Pinecone
```

### 3. **NormalizaciÃ³n de URLs**

El crawler normaliza URLs para evitar duplicados:

- `https://example.com/page`
- `https://example.com/page/` â† Se trata como la misma
- `https://example.com/page#section` â† Se ignora el fragmento

## ğŸ›¡ï¸ LÃ­mites de Seguridad

### LÃ­mite de PÃ¡ginas

Por defecto: **50 pÃ¡ginas mÃ¡ximo**

Esto previene:

- Crawling infinito en sitios muy grandes
- Uso excesivo de recursos
- Costos elevados en Pinecone

Para sitios mÃ¡s grandes, aumenta `max_pages`, pero considera:

- Tiempo de ejecuciÃ³n (mÃ¡s pÃ¡ginas = mÃ¡s tiempo)
- LÃ­mites de embeddings de Pinecone
- Costos de almacenamiento

### Timeout

Por defecto: **10 segundos por pÃ¡gina**

Si una pÃ¡gina tarda mÃ¡s, se salta y continÃºa con las demÃ¡s.

## ğŸ› SoluciÃ³n de Problemas

### "No content could be extracted"

- Verifica que el sitio sea accesible pÃºblicamente
- Algunos sitios bloquean scrapers (User-Agent)
- El sitio puede requerir JavaScript (no soportado)

### "Too many pages discovered"

- Aumenta `max_pages` en la configuraciÃ³n
- O divide el scraping en mÃºltiples ejecuciones con URLs mÃ¡s especÃ­ficas

### "Timeout errors"

- Aumenta el `timeout` si el sitio es lento
- Verifica tu conexiÃ³n a internet

## ğŸ“ Diferencias con el Sistema Anterior

| CaracterÃ­stica       | Antes      | Ahora                     |
| -------------------- | ---------- | ------------------------- |
| PÃ¡ginas scrapeadas   | Solo 1 URL | Todo el dominio           |
| Descubrimiento       | Manual     | AutomÃ¡tico                |
| Enlaces internos     | Ignorados  | Seguidos recursivamente   |
| LÃ­mite               | N/A        | 50 pÃ¡ginas (configurable) |
| TamaÃ±o de chunks     | 1000 chars | 2000 chars (duplicado)    |
| Filtrado de archivos | No         | SÃ­ (PDFs, imÃ¡genes, etc.) |

## ğŸ¯ Casos de Uso

### Blog o DocumentaciÃ³n

```python
# ScraperÃ¡ todos los artÃ­culos del blog
start_url = "https://mi-blog.com"
```

### Sitio de Productos

```python
# ScraperÃ¡ todas las pÃ¡ginas de productos
start_url = "https://mi-tienda.com/productos"
```

### DocumentaciÃ³n TÃ©cnica

```python
# ScraperÃ¡ toda la documentaciÃ³n
start_url = "https://docs.mi-proyecto.com"
```

## âš ï¸ Consideraciones Legales

- **Respeta el robots.txt** del sitio
- **Solo scrapea sitios que tienes permiso** para usar
- **Considera la carga** en el servidor objetivo
- **Verifica los tÃ©rminos de servicio** del sitio

## ğŸ”§ Mejoras Futuras Posibles

- [ ] Soporte para robots.txt
- [ ] Rate limiting configurable
- [ ] Soporte para autenticaciÃ³n
- [ ] Renderizado de JavaScript (Playwright/Selenium)
- [ ] Sitemap.xml parsing
- [ ] PriorizaciÃ³n de URLs
- [ ] CachÃ© de pÃ¡ginas visitadas

---

**Ãšltima actualizaciÃ³n**: 8 de noviembre de 2025
